{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Utils.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/msc-acse/acse-8-miniproject-entropy/blob/master/AlexNet.ipynb","timestamp":1558434984701}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kQ5dNHY-jThT","colab_type":"text"},"source":["# KMINST Classifier Utils"]},{"cell_type":"markdown","metadata":{"id":"_Dc5-7Nxjcaq","colab_type":"text"},"source":["## 1. Imports\n","These are important imports needed for the notebook to run"]},{"cell_type":"code","metadata":{"id":"Hlss1qj7kcon","colab_type":"code","outputId":"3989f872-8aa6-4ec0-e32d-e2ec2b5d3b69","executionInfo":{"status":"ok","timestamp":1558620373920,"user_tz":-60,"elapsed":5772,"user":{"displayName":"Adanna Akwataghibe","photoUrl":"","userId":"13306086339002156366"}},"colab":{"base_uri":"https://localhost:8080/","height":762}},"source":["!pip install pycm livelossplot\n","%pylab inline"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pycm in /usr/local/lib/python3.6/dist-packages (2.1)\n","Requirement already satisfied: livelossplot in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pycm) (1.16.3)\n","Requirement already satisfied: art>=1.8 in /usr/local/lib/python3.6/dist-packages (from pycm) (3.6)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot) (5.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot) (3.0.3)\n","Requirement already satisfied: coverage>=4.1 in /usr/local/lib/python3.6/dist-packages (from art>=1.8->pycm) (4.5.3)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (5.2.4)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.5.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (2.10.1)\n","Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (0.8.2)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.6.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (5.5.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.4.0)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (0.2.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.4.0)\n","Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot) (4.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (2.5.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (2.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot) (0.10.0)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->livelossplot) (17.0.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot) (1.1.1)\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->livelossplot) (0.6.0)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot) (5.5.0)\n","Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (3.1.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (1.4.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (2.1.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.6.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot) (0.4.2)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot) (2.6.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot) (4.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->livelossplot) (41.0.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (1.0.16)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (4.7.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.8.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.7.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot) (0.5.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot) (0.1.7)\n","Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xswk_3AWjwGK","colab_type":"text"},"source":["### CUDA setup"]},{"cell_type":"code","metadata":{"id":"UaMqMlYxkyye","colab_type":"code","outputId":"19bd3377-d9ef-445d-f785-51c8ea247b05","executionInfo":{"status":"ok","timestamp":1558620375355,"user_tz":-60,"elapsed":7189,"user":{"displayName":"Adanna Akwataghibe","photoUrl":"","userId":"13306086339002156366"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.naive_bayes import GaussianNB\n","\n","from scipy.stats import mode\n","\n","import numpy as np \n","import pandas as pd\n","\n","from livelossplot import PlotLosses\n","from pycm import *\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset \n","from torch.utils.data import TensorDataset, DataLoader\n","import torchvision.transforms as transforms\n","from torchvision.transforms import Compose, ToTensor, Normalize, RandomRotation, ToPILImage, RandomAffine, Resize, RandomChoice, RandomRotation, RandomHorizontalFlip\n","from torchvision.datasets import KMNIST\n","\n","\n","def set_seed(seed):\n","    \"\"\"\n","    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n","    torch.backends.cudnn.enabled   = False\n","\n","    return True\n","\n","device = 'cpu'\n","if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n","    print(\"Cuda installed! Running on GPU!\")\n","    device = 'cuda'\n","else:\n","    print(\"No GPU available!\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cuda installed! Running on GPU!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XLln-wppj1Ma","colab_type":"text"},"source":["### Mounting Google drive"]},{"cell_type":"code","metadata":{"id":"Q6XzyOU-lGW3","colab_type":"code","outputId":"b8ffa055-95c8-4803-ce53-e14b01a8895e","executionInfo":{"status":"ok","timestamp":1558620375357,"user_tz":-60,"elapsed":7171,"user":{"displayName":"Adanna Akwataghibe","photoUrl":"","userId":"13306086339002156366"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z_LUL4Wa-QV6","colab_type":"text"},"source":["### Download the KMNIST Data from Google drive"]},{"cell_type":"code","metadata":{"id":"uA4qywgmnt7f","colab_type":"code","colab":{}},"source":["np_kmnist_data = np.load(\"/content/gdrive/My Drive/kmnist_data/kmnist-train-imgs.npy\")\n","np_kmnist_labels = np.load(\"/content/gdrive/My Drive/kmnist_data/kmnist-train-labels.npy\")\n","np_kmnist_test = np.load(\"/content/gdrive/My Drive/kmnist_data/kmnist-test-imgs.npy\")\n","\n","# This converts numpy to tensor\n","kmnist_data = torch.tensor(np_kmnist_data)\n","kmnist_labels = torch.tensor(np_kmnist_labels)\n","kmnist_test = torch.tensor(np_kmnist_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XGO-TINEHbz","colab_type":"text"},"source":["## 2. Mean, Standard deviation and Manual Normalisation"]},{"cell_type":"code","metadata":{"id":"OiAzi5w8CWPa","colab_type":"code","colab":{}},"source":["def get_mean_std(train, val):\n","    \"\"\"\n","    Gets the mean and std of a training and validation set\n","    \n","    Input: train - a torch.Tensor object\n","           val - a torch.Tensor object\n","           \n","    Returns: train_mean, val_mean, train_std, val_std (floats) - mean and standard deviations of train and val\n","    \"\"\"\n","    train = np.asarray(train)\n","    val = np.asarray(val)\n","    \n","    train_mean = train.mean() / 255 \n","    val_mean = val.mean() / 255\n","    \n","    train_std = train.std() / 255\n","    val_std = val.std() / 255\n","  \n","    return train_mean, val_mean, train_std, val_std\n","\n","\n","def apply_normalization(X):\n","    '''\n","    This normalization function will normalize each individual image based on the mean and stdev of the entire dataset.\n","    Input: X - A torch.Tensor object\n","    \n","    Returns: X - torch.Tensor object\n","    '''\n","    # takes an average of the image channel, i\n","    mean = X.mean()\n","    # takes a stdev of the image channel, i\n","    std = X.std()\n","    X -= mean\n","    X /= std\n","    return X"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3hROGceb-sPZ","colab_type":"text"},"source":["## 3. Data Augmentation Setup"]},{"cell_type":"markdown","metadata":{"id":"ekW5TxuFod-T","colab_type":"text"},"source":["### CustomTensorDataSet"]},{"cell_type":"code","metadata":{"id":"2s2SHyhFDBu9","colab_type":"code","colab":{}},"source":["class CustomImageTensorDataset(Dataset):\n","    def __init__(self, data, targets, transform=None):\n","        \"\"\"\n","        Args:\n","            data (Tensor): A tensor containing the data e.g. images\n","            targets (Tensor): A tensor containing all the labels\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.data = data\n","        self.targets = targets\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample, label = self.data[idx], self.targets[idx]\n","        sample = sample.view(1, 28, 28).float()/255.\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample, label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kpxPUGFboxFi","colab_type":"text"},"source":["### Transformations"]},{"cell_type":"code","metadata":{"id":"-D3Pl-2TowCQ","colab_type":"code","colab":{}},"source":["def transformed(mean, std, choice=0):\n","    \"\"\"\n","    This function performs the composition of transformations depending on the choice of given\n","\n","    Input: mean - float\n","           std - float\n","           choice - value of 0, 1 or 2 which chooses the type of transformation that is applied to the dataset\n","    \"\"\"\n","    transform = Compose([\n","        ToPILImage(),\n","        RandomAffine(degrees=10., translate=(0.1, 0.1), shear=10.),\n","        ToTensor(),\n","        Normalize(mean=[mean], std=[std])\n","    ])\n","\n","    if choice == 1:\n","        transform = Compose([\n","            ToPILImage(),\n","            ToTensor(),\n","            Normalize(mean=[mean], std=[std])\n","        ])\n","\n","    if choice == 2:\n","        transform = Compose([\n","            ToPILImage(),\n","            RandomChoice([\n","                RandomRotation(10),\n","                RandomHorizontalFlip(1.0),\n","                RandomAffine(degrees=0, translate=(0.1, 0.1), shear=0.),\n","                RandomAffine(degrees=0, translate=(0., 0.), shear=10.)\n","            ]),\n","            ToTensor(),\n","            Normalize(mean=[mean], std=[std])\n","        ])\n","\n","    return transform\n","\n","\n","# a random rotation transform to be used on a split training dataset\n","train__rotate = Compose([\n","    ToPILImage(),\n","    RandomRotation(10),\n","    ToTensor(),\n","    Normalize(mean=[0.1919], std=[0.3483])\n","])\n","\n","# a random affine transform (includes a random rotation, translation and shear) to be used on a split training dataset\n","train__random = Compose([\n","    ToPILImage(),\n","    RandomAffine(degrees=10., translate=(0.1,0.1),shear=10.),\n","    ToTensor(),\n","    Normalize(mean=[0.1919], std=[0.3483])\n","])\n","\n","# a random rotation transform to be used on the full kmnist dataset\n","full_train__rotate = Compose([\n","    ToPILImage(),\n","    RandomRotation(10),\n","    ToTensor(),\n","    Normalize(mean=[0.1919], std=[0.3483])\n","])\n","\n","# a random affine transform to be used on the full kmnist dataset\n","full_train__random = Compose([\n","    ToPILImage(),\n","    RandomAffine(degrees=10., translate=(0.1,0.1),shear=10.),\n","    ToTensor(),\n","    Normalize(mean=[0.1919], std=[0.3483])\n","])\n","\n","# a normalization transform to be used on a split validation dataset\n","validation_test_transform = Compose([\n","    Normalize(mean=[0.1919], std=[0.3486])\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nFtOpcyEiTb","colab_type":"text"},"source":["## 4.  K-fold Cross Validation"]},{"cell_type":"code","metadata":{"id":"vK6zSGTU4aWD","colab_type":"code","colab":{}},"source":["def k_split(X, y, splits=5):\n","  \"\"\"\n","  This function splits the training set into k-folds to use for hyperparamter optimisation\n","  \n","  Input: X - The training set, a torch.Tensor object\n","         y - The training labels, a torch.Tensor object\n","         splits - default = 5, number of folds, Integer\n","         \n","  Returns: trains - List of training sets, list is length of k,  List of torch.Tensor objects\n","           vals - List of validation sets, list is length of k,  List of torch.Tensor objects\n","           train_labels - List of training labels, list is length of k,  List of torch.Tensor objects\n","           val_labels - List of validation labels, list is length of k,  List of torch.Tensor objects\n","\n","  \"\"\"\n","  \n","  if(splits==1): # Just split training and validation without k-fold \n","    shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(X, y)\n","    indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]\n","\n","    X_train, y_train = X[indices[0]], y[indices[0]]\n","    X_val, y_val = X[indices[1]], y[indices[1]]\n","    \n","    return X_train, X_val, y_train, y_val\n","  \n","  # Lists of training sets and labels and validation set and labels\n","  trains = []\n","  vals = [] \n","  train_labels = [] \n","  val_labels = []\n","  \n","  skf = StratifiedKFold(n_splits=splits, random_state=None)\n","  \n","  for train_index, val_index in skf.split(X,y): \n","      X_train, X_val = X[train_index], X[val_index] \n","      y_train, y_val = y[train_index], y[val_index]\n","      \n","      trains.append(X_train)\n","      vals.append(X_val)\n","      train_labels.append(y_train)\n","      val_labels.append(y_val)\n","      \n","  return trains, vals, train_labels, val_labels\n","\n","\n","def k_fold_optimisation(k_folds, momentums=None, weight_decays=None, lrs=None, epochs=None, optims=None):\n","  \"\"\"\n","  This function performs k-fold cross validation \n","  \n","  Input: k_folds - Number of folds, Integer\n","         momentum - List of momentum values for each fold, List of floats\n","         weight_decays - List of weight decay values for each fold, List of floats\n","         lrs - List of learning rate values for each fold, List of floats\n","         epochs - List of epoch values for each fold, List of integers\n","         weight_decays - List of weight decay values for each fold, List of floats\n","  \n","  \"\"\"\n","\n","  # Check that we have chosen at least one parameter to test\n","  if momentum == None and weight_decays == None and lrs == None and epochs == None and optims == None:\n","    print(\"Error: No parameter list!\")\n","    return\n","  \n","  if k_folds < 2:\n","    print(\"Error: Must have more than 1 fold\")\n","    return\n","  \n","  # Whatever parameters is chosen must be equal to number of k-folds\n","  assert(k_folds == len(momentums) or k_folds == len(weight_decays) or k_folds == len(lrs) or k_folds == len(epochs) or k_folds == len(optims))\n","  \n","  # Split using k-fold\n","  trains, valids, tr_labels, val_labels = k_split(kmnist_data, kmnist_labels, splits=k_folds)\n","  \n","  # Save the current sum of validation accuracies for the respective index of the parameters\n","  sum_accs = k_folds * [0.0]\n","  \n","  # Train each training set + test with validation set\n","  for i in range(len(trains)):\n","    train_mean, val_mean, train_std, val_std = get_mean_std(trains[i], valids[i])\n","    \n","    k_train = CustomTensorDataset(trains[i], tr_labels[i].long(), transform=transformed(train_mean, train_std))\n","    k_validate = CustomTensorDataset(valids[i], val_labels[i].long(), transform=transformed(val_mean, val_std))\n","  \n","    # Train with the different parameters\n","    for j in range(k_folds):\n","      params = None\n","      if len(momentums) == k_folds:\n","        params = {momentum: momentums[j]}\n","      elif len(weight_decays) == k_folds:\n","        params = {weight_decay: weight_decays[j]}\n","      elif len(lrs) == k_folds:\n","        params = {lr: lrs[j]}\n","      elif len(epochs) == k_folds:\n","        params = {n_epochs: epochs[j]}\n","      elif len(optims) == k_folds:\n","         params = {optim: optims[j]}\n","        \n","      model, train_accs, train_losses, valid_accs, valid_losses = train_model_params(k_train, k_validate, params)\n","      \n","      sum_accs[j] += max(valid_accs)\n","      \n","  # Get the average validation accuracy for each parameter \n","  sum_accs /= k_folds\n","  \n","  # Get the index of the best parameter\n","  indx_of_max = np.argmax(sum_accs)\n","    \n","  if len(momentums) == k_folds:\n","    return momentums[indx_of_max], max(sum_accs)\n","  elif len(weight_decays) == k_folds:\n","    return weight_decays[indx_of_max], max(sum_accs)\n","  elif len(lrs) == k_folds:\n","    return lrs[indx_of_max], max(sum_accs)\n","  elif len(epochs) == k_folds:\n","    return epochs[indx_of_max], max(sum_accs)\n","  elif len(optims) == k_folds:\n","     return optims[indx_of_max], max(sum_accs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7uDejaQuIBT","colab_type":"text"},"source":["## 5. Model Training Functions\n"]},{"cell_type":"markdown","metadata":{"id":"eqym4eqM1A8f","colab_type":"text"},"source":["### Parameters kept constant\n","We decided to keep some hyperparameters constant in order to reduce the number of parameters to optimise"]},{"cell_type":"code","metadata":{"id":"MGBsszlj09s8","colab_type":"code","colab":{}},"source":["seed = 42\n","batch_size = 64\n","test_batch_size = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-GbYrFd04g_","colab_type":"text"},"source":["### Train, Validate and Evaluate"]},{"cell_type":"code","metadata":{"id":"lfVbeicGuOTl","colab_type":"code","colab":{}},"source":["def train(model, optimizer, criterion, data_loader):\n","    \"\"\"\n","    This function trains a neural network based on a selected optimizer and loss function\n","    \n","    Input: model - a torch.nn class, can be a custom class. Ex. LeNet5 below\n","           optimizer - a pytorch optim function. Ex. torch.optim.SGD\n","           criterion - a torch.nn class. Ex. nn.CrossEntropyLoss()\n","           data_loader - a pytorch DataLoader initialized with a training and validation/test set\n","    \n","    Returns: training losses and training accuracy (floats)\n","    \"\"\"\n","    model.train()\n","    train_loss, train_accuracy = 0, 0\n","    for X, y in data_loader:\n","        X, y = X.to(device), y.to(device)\n","        optimizer.zero_grad()\n","        a2 = model(X.view(-1, 1, 28, 28))\n","        loss = criterion(a2, y)\n","        loss.backward()\n","        train_loss += loss*X.size(0)\n","        y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0)\n","        optimizer.step()  \n","        \n","    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n","\n","\n","def validate(model, criterion, data_loader):\n","    \"\"\"\n","    This function validates a neural network based on a selected optimizer and loss function\n","    \n","    Input: model - a torch.nn class, can be a custom class. Ex. LeNet5 below\n","           optimizer - a pytorch optim function. Ex. torch.optim.SGD\n","           criterion - a torch.nn class. Ex. nn.CrossEntropyLoss()\n","           data_loader - a pytorch DataLoader initialized with a training and validation/test set\n","    \n","    Returns: validation losses and validation accuracy (floats)\n","    \"\"\"\n","    \n","    model.eval()\n","    validation_loss, validation_accuracy = 0., 0.\n","    for X, y in data_loader:\n","        with torch.no_grad():\n","            X, y = X.to(device), y.to(device)\n","            a2 = model(X.view(-1, 1, 28, 28))\n","            loss = criterion(a2, y)\n","            validation_loss += loss*X.size(0)\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0)\n","            \n","    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n","\n","\n","def evaluate(model, X_test):\n","    \"\"\"\n","    This function labels an input image based on a Softmax of a neural network output \n","    \n","    Input: model - a torch.nn class, can be a custom class. Ex. LeNet5 below\n","           X_test - a pytorch.Tensor object\n","    \n","    Returns: y_preds - a list of the labels for each image in X\n","    \"\"\"\n","    model.eval()\n","    y_preds = []\n","    for X in X_test:\n","        with torch.no_grad():\n","            X = X.to(device)\n","            a2 = model(X.view(-1, 1, 28, 28))\n","            y_pred = F.log_softmax(a2, dim=1).max(1)[1]\n","            y_preds.append(y_pred.cpu().numpy())\n","            \n","    return np.concatenate(y_preds, 0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2IugjQfuqzR","colab_type":"text"},"source":["### Training functions"]},{"cell_type":"code","metadata":{"id":"_8e6htsku1Gu","colab_type":"code","colab":{}},"source":["def train_model(model, trainset, validset, lr=1e-2, momentum=0.5, weight_decay=0.0):\n","    \"\"\"\n","    This function trains a model through the a number of epoch cycles and optimizes the model parameters based on the loss function\n","\n","    Input: trainset - a pytorch Dataset initialized with a training data Tensor\n","           validset - a pytorch Dataset initialized with validation/test data Tensor\n","           lr - learning rate used by model, float\n","           momentum - a parameter specified in the SGD optimizer, float\n","           weight_decay - a regularization parameter specified in the SGD optimizer, float\n","\n","    Returns: model - a torch.nn class\n","    \"\"\"\n","  \n","    set_seed(seed)\n","    model = model.to(device)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n","    validation_loader = DataLoader(validset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","\n","    liveloss = PlotLosses()\n","\n","    for epoch in range(n_epochs):\n","        logs = {}\n","        train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","        logs['' + 'log loss'] = train_loss.item()\n","        logs['' + 'accuracy'] = train_accuracy.item()\n","\n","        validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","\n","        logs['val_' + 'log loss'] = validation_loss.item()\n","        logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","        liveloss.update(logs)\n","        liveloss.draw()\n","\n","    return model\n","\n","\n","\n","def train_model_params(model, trainset, validset, full=False, plot=True, params=None):\n","    \"\"\"\n","    This function trains a model through the a number of epoch cycles and optimizes the model parameters based on the loss function\n","    \n","    Input: trainset - a pytorch Dataset initialized with a training data Tensor\n","           validset - a pytorch Dataset initialized with validation/test data Tensor\n","           full - if True, then train on whole training set and validate on test set\n","           plot - if True, will display a livelossplot of the training and validation losses/accuracies. Else, prints the losses and accuracies\n","           params - dict of parameter values {momentum, weight_decay, lr, n_epochs, optim}. If parameter is not defined when called, then \n","                    default values (below) are used\n","    \n","    Returns: model - a torch.nn class\n","             train_accs - list of training accuracy scores, list of floats\n","             train_losses - list of training losses, list of floats\n","             valid_accs - list of validation accuracy scores, list of floats\n","             valid_losses - list of validation losses, list of floats\n","    \"\"\"\n","  \n","    # Default parameters\n","    momentum = 0.5 \n","    weight_decay = 0.0\n","    lr = 1e-2\n","    n_epochs = 30 \n","    optim = 'SGD'\n","\n","    if params: # If params is set with user defined values\n","      momentum = params.get('momentum') if params.get('momentum') else momentum\n","      weight_decay = params.get('weight_decay') if params.get('weight_decay') else weight_decay\n","      lr = params.get('lr') if params.get('lr') else lr\n","      n_epochs = params.get('n_epochs') if params.get('n_epochs') else n_epochs\n","      optim = params.get('optim') if params.get('optim') else optim\n","\n","    print(\"Set parameters:\", \"momentum:\", momentum, \"weight decay:\", weight_decay, \"learning rate:\", lr, \"number of epochs:\", n_epochs, \"optimiser:\", optim, \"plot:\", plot)\n","\n","    set_seed(seed)\n","    model = model.to(device)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    if optim == 'Adam': # Adam optimiser\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","        print(\"Adam Optimiser used\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n","    validation_loader = DataLoader(validset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","\n","    liveloss = PlotLosses()\n","\n","    # Save the training acc and losses \n","    train_accs, train_losses = [], []\n","\n","    # Save the validation acc and losses\n","    valid_accs, valid_losses = [], []\n","\n","    for epoch in range(n_epochs):\n","        print(\"epoch: \", epoch)\n","        logs = {}\n","        train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","\n","        train_accs.append(train_accuracy)\n","        train_losses.append(train_loss)\n","\n","        if plot:\n","          logs['' + 'log loss'] = train_loss.item()\n","          logs['' + 'accuracy'] = train_accuracy.item()\n","\n","\n","        if not full:\n","\n","          validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","\n","          valid_accs.append(validation_accuracy)\n","          valid_losses.append(validation_loss)\n","\n","          if plot:\n","            logs['val_' + 'log loss'] = validation_loss.item()\n","            logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","        if full:\n","          if plot:\n","            logs['val_' + 'log loss'] = 0\n","            logs['val_' + 'accuracy'] = 0\n","\n","        liveloss.update(logs)\n","        liveloss.draw()\n","\n","    if full:\n","      test_loss, test_accuracy = validate(model, criterion, validation_loader)    \n","      print(\"Avg. Test Loss: %1.3f\" % test_loss.item(), \" Avg. Test Accuracy: %1.3f\" % test_accuracy.item())\n","      print(\"\")\n","\n","    return model, train_accs, train_losses, valid_accs, valid_losses\n","\n","\n","\n","def train_model_augmented(model, train_dataset, validation_dataset, aug_dataset, lr=1e-2, momentum=0.5, weight_decay=0., n_epochs=30, plot=True, augs=1):\n","    \"\"\"\n","    This function trains a model through the a number of epoch cycles and optimizes the model parameters based on the loss function\n","    \n","    Input: model - a torch.nn class, can be a custom class. Ex. LeNet5 below\n","           train_dataset - a pytorch Dataset initialized with a training data Tensor\n","           validation_dataset - a pytorch Dataset initialized with validation/test data Tensor\n","           lr - learning rate used by model, float\n","           momentum - a parameter specified in the SGD optimizer, float\n","           weight_decay - a regularization parameter specified in the SGD optimizer, float\n","           n_epochs - the total number of epoch cycles to train the model, integer\n","           plot - if True, will display a livelossplot of the training and validation losses/accuracies. Else, prints the losses and accuracies\n","           augs - number of extra augumented trainings per epoch cycle\n","    \n","    Returns: model - a torch.nn class\n","    \"\"\"\n","    set_seed(seed)\n","    model = model.to(device)\n","    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","    validation_loader = DataLoader(validation_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0)\n","    aug_loader = DataLoader(aug_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","    if plot:\n","      liveloss = PlotLosses()\n","      for epoch in range(n_epochs):\n","          logs = {}\n","          # train on original dataset\n","          train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","          # train on augmented dataset\n","          for i in range(augs):\n","              train_loss, train_accuracy = train(model, optimizer, criterion, aug_loader)\n","\n","              logs['' + 'log loss'] = train_loss.item()\n","              logs['' + 'accuracy'] = train_accuracy.item()\n","\n","              validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","              logs['val_' + 'log loss'] = validation_loss.item()\n","              logs['val_' + 'accuracy'] = validation_accuracy.item()\n","\n","              liveloss.update(logs)\n","              liveloss.draw()\n","    else:\n","      for epoch in range(n_epochs):\n","          # train on original dataset\n","          train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n","          # train on augmented dataset\n","          for i in range(augs):\n","              train_loss, train_accuracy = train(model, optimizer, criterion, aug_loader)\n","\n","          validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n","\n","          if (epoch % 5 == 0 or epoch == (n_epochs - 1)):\n","              print(\"Training loss: \", train_loss.item())\n","              print(\"Val loss: \", validation_loss.item())\n","              print(\"Training acc: \", train_accuracy.item())\n","              print(\"Val acc: \", validation_accuracy.item())\n","\n","    return model\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UHFknl3ywfs","colab_type":"text"},"source":["## 6. Save Predictions as CSV file"]},{"cell_type":"code","metadata":{"id":"ic00x_11y9nN","colab_type":"code","colab":{}},"source":["def save_predictions(model, X_test, name=None):\n","    \"\"\"\n","    Saves the predictions of the model from the test set as a csv file \n","\n","    Input: model - a torch.nn class, can be a custom class. Ex. LeNet, AlexNet\n","           X_test - testset, a pytorch.Tensor object\n","    \"\"\"\n","    X_test = apply_normalization(X_test)\n","    y_pred1 = evaluate(model, X_test)\n","\n","    ID = np.arange(0, len(y_pred1))\n","    dataframe1 = pd.DataFrame({'ID': ID, 'Category': y_pred1})\n","\n","    # Saves to CSV file\n","    if not name:\n","      name = \"kmnist_classifier.csv\"\n","    path = F\"/content/gdrive/My Drive/KMNIST_ENTROPY/results/{name}\"\n","    dataframe1.to_csv(path, index=False, sep=',')\n","    \n","    \n","def save_predictions_ns(y_preds, name=None):\n","    \"\"\"\n","    Saves the predictions of classifier given as a list as a csv file \n","\n","    Input: y_pred - numpy array\n","    \"\"\"\n","    \n","    ID = np.arange(0, len(y_preds))\n","    dataframe1 = pd.DataFrame({'ID': ID, 'Category': y_preds})\n","\n","    # Saves to CSV file\n","    if not name:\n","      name = \"kmnist_pca_classifier.csv\"\n","    path = F\"/content/gdrive/My Drive/KMNIST_ENTROPY/results/{name}\"\n","    dataframe1.to_csv(path, index=False, sep=',')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ak4pdKJ24q_c","colab_type":"text"},"source":["## 7. Save Model"]},{"cell_type":"code","metadata":{"id":"ds_Jq44N4urk","colab_type":"code","colab":{}},"source":["def save_model(model, name):\n","  \"\"\"\n","  Saves the model as a pt file \n","  \n","  Input: model - a torch.nn class\n","         name - file name, a string\n","  \"\"\"\n","  \n","  # Ex. model_save_name = 'AlexNet_kmnist_classifier_random_ep60_drop0_wd1e3.pt'\n","  path = F\"/content/gdrive/My Drive/KMNIST_ENTROPY/models/{name}\" \n","  torch.save(model.state_dict(), path)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rXYl6WsvApCO","colab_type":"text"},"source":["## 8. Load Model "]},{"cell_type":"code","metadata":{"id":"_O5nOh6eArYH","colab_type":"code","colab":{}},"source":["  def load_model(model, name):\n","    \"\"\"\n","    Loads the model as a pt file\n","\n","    Input: model - a torch.nn class\n","           name - file name, a string\n","    \"\"\"\n","\n","    model = model.to(device)\n","    model.load_state_dict(torch.load(F\"/content/gdrive/My Drive/KMNIST_ENTROPY/models/{name}\"))\n","    model.to(device)\n","  \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5pveWE_JUEv","colab_type":"code","colab":{}},"source":["print(\"Utils.ipynb has finished downloading\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R96gmB9YBohX","colab_type":"text"},"source":[""]}]}